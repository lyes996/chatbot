# Changelog - Confluence Chatbot

## Version 2.0.0 - Migration vers OpenAI (2025-11-05)

### üöÄ Changements majeurs

#### Remplacement d'Ollama par OpenAI
- **Raison** : Ollama n√©cessite un serveur local et n'est pas compatible avec les plateformes serverless comme Vercel
- **Solution** : Migration vers OpenAI API pour les embeddings et la g√©n√©ration de r√©ponses
- **Avantages** :
  - ‚úÖ Compatible avec Vercel (serverless)
  - ‚úÖ Pas besoin de serveur local
  - ‚úÖ Meilleure qualit√© de r√©ponses
  - ‚úÖ D√©ploiement simplifi√©
  - ‚úÖ Scalabilit√© automatique

### üìù Fichiers modifi√©s

#### Nouveaux fichiers
- `lib/ai.ts` - Int√©gration OpenAI (remplace `lib/ollama.ts`)
- `DEPLOYMENT_GUIDE.md` - Guide complet de d√©ploiement
- `VERCEL_SETUP.md` - Configuration rapide pour Vercel
- `QUICK_START.md` - Guide de d√©marrage rapide (15 min)
- `CHANGELOG.md` - Ce fichier
- `vercel.json` - Configuration Vercel
- `scripts/check-env.js` - V√©rification des variables d'environnement

#### Fichiers modifi√©s
- `lib/rag.ts` - Import mis √† jour (ollama ‚Üí ai)
- `app/api/chat/route.ts` - Import mis √† jour
- `app/api/health/route.ts` - V√©rification OpenAI au lieu d'Ollama
- `scripts/ingest-confluence.js` - Utilisation d'OpenAI pour les embeddings
- `scripts/setup-database.js` - Dimension des vecteurs : 4096 ‚Üí 1536 (OpenAI)
- `package.json` - D√©pendance : `ollama` ‚Üí `openai`
- `.env.example` - Variables OpenAI au lieu d'Ollama
- `README.md` - Documentation mise √† jour

#### Fichiers supprim√©s
- `lib/ollama.ts` - Remplac√© par `lib/ai.ts`

### üîß Changements techniques

#### Embeddings
- **Avant** : Ollama avec dimension 4096
- **Apr√®s** : OpenAI `text-embedding-3-small` avec dimension 1536
- **Impact** : Base de donn√©es mise √† jour pour supporter vector(1536)

#### Mod√®les de chat
- **Avant** : llama2, mistral, mixtral (local)
- **Apr√®s** : gpt-3.5-turbo, gpt-4, gpt-4-turbo (cloud)
- **Configuration** : Variable `OPENAI_MODEL` dans `.env`

#### Variables d'environnement

**Supprim√©es** :
```
OLLAMA_BASE_URL
OLLAMA_MODEL
```

**Ajout√©es** :
```
OPENAI_API_KEY
OPENAI_MODEL
```

### üìä Comparaison

| Aspect | Ollama (v1) | OpenAI (v2) |
|--------|-------------|-------------|
| H√©bergement | Local | Cloud |
| Vercel | ‚ùå Non compatible | ‚úÖ Compatible |
| Setup | Complexe | Simple |
| Co√ªt | Gratuit | ~$1-5/mois |
| Qualit√© | Bonne | Excellente |
| Latence | Variable | Stable |
| Scalabilit√© | Limit√©e | Illimit√©e |

### üéØ Migration

Pour migrer d'une installation existante :

1. **Mettre √† jour les d√©pendances** :
   ```bash
   npm install
   ```

2. **Mettre √† jour `.env`** :
   ```bash
   # Supprimer
   OLLAMA_BASE_URL=...
   OLLAMA_MODEL=...
   
   # Ajouter
   OPENAI_API_KEY=sk-...
   OPENAI_MODEL=gpt-3.5-turbo
   ```

3. **Recr√©er la base de donn√©es Supabase** :
   - Ex√©cuter le nouveau SQL (vector(1536) au lieu de vector(4096))
   - Voir `DEPLOYMENT_GUIDE.md` pour le SQL complet

4. **R√©ing√©rer les donn√©es** :
   ```bash
   npm run ingest
   ```

5. **Red√©ployer sur Vercel** :
   - Mettre √† jour les variables d'environnement
   - Red√©ployer

### ‚ö†Ô∏è Breaking Changes

- Les embeddings existants ne sont **pas compatibles** (dimension diff√©rente)
- N√©cessite une **r√©ingestion compl√®te** des donn√©es
- Les variables d'environnement ont chang√©
- N√©cessite une **cl√© API OpenAI** (payante)

### üêõ Corrections

- Gestion d'erreur am√©lior√©e pour les variables d'environnement manquantes
- Validation des credentials avant l'ingestion
- Messages d'erreur plus clairs

### üìö Documentation

- Guide de d√©ploiement complet ajout√©
- Guide de d√©marrage rapide (15 min)
- Configuration Vercel document√©e
- Script de v√©rification des variables d'environnement

### üîú Prochaines √©tapes

- [ ] Support multi-espaces Confluence
- [ ] Cache des embeddings pour r√©duire les co√ªts
- [ ] Interface d'administration
- [ ] M√©triques et analytics
- [ ] Support de fichiers attach√©s
- [ ] Recherche hybride (keyword + semantic)

---

## Version 1.0.0 - Version initiale (2025-11-04)

### Fonctionnalit√©s

- Architecture RAG compl√®te
- Int√©gration Confluence
- Ollama pour les embeddings et le chat
- Supabase avec pgvector
- Interface Next.js
- Streaming des r√©ponses
- Affichage des sources

### Limitations

- N√©cessite un serveur local pour Ollama
- Non compatible avec Vercel
- Setup complexe
